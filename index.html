<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ZHOUHAN LIN</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
    <link rel="stylesheet" href="./index.css">
</head>

<body>
    <div>
        <!-- This is header -->
        <header class="home-header">
            <div class="header-content row d-flex flex-wrap align-items-center justify-content-between">
                <div class="header-logo col-xs-12 col-sm-4">ZHOUHAN LIN</div>
                <ul class="col-xs-12 col-sm-8 col-md-auto m-0 d-flex flex-wrap align-items-center justify-content-center justify-content-md-between list-unstyled">

                    <li class="px-4 mx-16">
                        <div class="navbar-header">
                            <a class="navbar-brand" href="#bio">Bio</a>
                        </div>
                    </li>
                    <li class="px-4 mx-16">
                        <div class="navbar-header">
                            <a class="navbar-brand" href="#research">Research Interests</a>
                        </div>
                    </li>
                    <li class="px-4 mx-16">
                        <div class="navbar-header">
                            <a class="navbar-brand" href="#news">News</a>
                        </div>
                    </li>
                    <!-- 在这里新增header navigation，格式如下： -->
                    <!-- <li class="px-4 mx-16">
                        <div class="navbar-header">
                            <a class="navbar-brand" href="#publications">Publications</a>
                        </div>
                    </li> -->
                </ul>
            </div>
        </header>
        <section class="section-container">
            <!--This is Bio -->
            <!-- 这里的id="bio"，需要与header中的href="#bio"对应，用来点击滑动定位 -->
            <section id="bio" class="section-item">
                <div class="row">
                    <div class="col-xs-12 col-sm-4">
                        <img src="https://hantek.github.io/static/img/coverphoto.jpg" alt="" srcset="" width="80%">
                    </div>
                    <div class="col-xs-12 col-sm-8">
                        <h2 style="margin-bottom: 24px;">Zhouhan Lin (林洲汉)</h2>
                        <div>
                            <span style="font-weight: 500;">E-mail :</span> lastname[dot]firstname[at]gmail.com
                        </div>
                        <p>
                            <span style="font-weight: 500;">Phone :</span> 650-788-4556
                        </p>
                        <p>
                            Hello! I'm Zhouhan Lin, a visiting scientist at Facebook AI Research. In 2021, I am going to join Shanghai Jiaotong University as an assistant professor. I graduated from the Mila lab in the University of Montreal, where I have the honor to be supervised
                            by Yoshua Bengio.
                        </p>
                        <p>
                            My research interests include machine learning and natural language processing, especially in attention mechanisms and its applications, language modeling, question answering, syntactic parsing, and binary networks.
                        </p>

                        <!-- 想要另起段落，直接新建一个p标签即可，如下 -->
                        <!-- <p>这是新段落</p> -->

                        <p class="link-block">
                            <a href="http://" target="_blank">
                                <img src="./assets/facebook.svg" alt="" srcset="">
                            </a>
                            <a href="http://" target="_blank">
                                <img src="./assets/twitter.svg" alt="" srcset="">
                            </a>
                            <a href="http://" target="_blank">
                                <img src="./assets//linkedin.svg" alt="" srcset="">
                            </a>
                            <a href="http://" target="_blank">
                                <img src="./assets//zhihu.svg" alt="" srcset="">
                            </a>
                        </p>
                    </div>
                </div>
            </section>
            <!-- This is research -->
            <section id="research" class="section-item">
                <h3 style="margin: 0;">
                    Research Interests
                </h3>
                <section class="research-section">
                    <!-- research 1 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/distanceparser.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Straight to the Tree: Constituency Parsing with Neural Syntactic Distance
                            </h4>
                            <p class="author">
                                Yikang Shen*, Zhouhan Lin*, Athul Paul Jacob, Alessandro Sordoni, Aaron Courville, Yoshua Bengio
                            </p>
                            <div class="link-list">
                                <span>ACL 2018 </span>
                                <span> | </span>
                                <a href="http://"><span>pdf</span></a>
                                <span> | </span>
                                <a href="http://"><span>slides</span></a>
                                <span> | </span>
                                <a href="http://"><span>codes</span></a>
                            </div>
                            <abstract>
                                We propose a novel constituency parsing scheme. The model predicts a vector of real-valued scalars, named syntactic distances, for each split position in the input sentence. The syntactic distances specify the order in which the split points will be selected,
                                recursively partitioning the input, in a top-down fashion. Compared to traditional shiftreduce parsing schemes, our approach is free from the potential problem of compounding errors, while being faster and easier to parallelize.
                                Our model achieves competitive performance amongst single model, discriminative parsers in the PTB dataset and outperforms previous models in the CTB dataset.
                            </abstract>
                        </div>
                    </div>

                    <!-- research 2 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/rrnn.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Learning Hierarchical Structures On-The-Fly with a Recurrent-Recursive Model for Sequences
                            </h4>
                            <p class="author">
                                Athul Paul Jacob*, Zhouhan Lin*, Alessandro Sordoni, Yoshua Bengio
                            </p>
                            <div class="link-list">
                                <span>ACL 2018 workshop</span>
                                <span> | </span>
                                <a href="http://"><span>pdf</span></a>
                                <span> | </span>
                                <a href="http://"><span>poster</span></a>
                            </div>
                            <abstract>
                                We propose a hierarchical model for sequential data that learns a tree on-the-fly, i.e. while reading the sequence. In the model, a recurrent network adapts its structure and reuses recurrent weights in a recursive manner. This creates adaptive skip-connections
                                that ease the learning of long-term dependencies. The tree structure can either be inferred without supervision through reinforcement learning, or learned in a supervised manner. We provide preliminary experiments in a
                                novel Math Expression Evaluation (MEE) task, which is explicitly crafted to have a hierarchical tree structure that can be used to study the effectiveness of our model. Additionally, we test our model in a wellknown propositional
                                logic and language modelling tasks. Experimental results show the potential of our approach.
                            </abstract>
                        </div>
                    </div>
                    <!-- research 3 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/tree.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Neural Language Modeling by Jointly Learning Syntax and Lexicon
                            </h4>
                            <p class="author">
                                Yikang Shen, Zhouhan Lin, Chin-Wei Huang, Aaron Courville
                            </p>
                            <div class="link-list">
                                <span>ICLR 2018</span>
                                <span> | </span>
                                <a href="http://"><span>pdf</span></a>
                                <span> | </span>
                                <a href="http://"><span>slides</span></a>
                                <span> | </span>
                                <a href="http://"><span>codes</span></a>
                                <span> | </span>
                                <a href="http://"><span>poster</span></a>
                            </div>
                            <abstract>
                                We propose a neural language model capable of unsupervised syntactic structure induction. The model leverages the structure information to form better semantic representations and better language modeling. Standard recurrent neural networks are limited
                                by their structure and fail to efficiently use syntactic information. On the other hand, tree-structured recursive networks usually require additional structural supervision at the cost of human expert annotation. In this
                                paper, We propose a novel neural language model, called the Parsing-Reading-Predict Networks (PRPN), that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure
                                to learn a better language model. In our model, the gradient can be directly back-propagated from the language model loss into the neural parsing network. Experiments show that the proposed model can discover the underlying
                                syntactic structure and achieve state-of-the-art performance on word/character-level language model tasks.
                            </abstract>
                        </div>
                    </div>
                    <!-- research 4 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/semlp.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                A structured self-attentive Sentence Embedding
                            </h4>
                            <p class="author">
                                Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou and Yoshua Bengio
                            </p>
                            <div class="link-list">
                                <span class="time">ICLR 2017</span>
                                <span> | </span>
                                <a href="http://"><span>pdf</span></a>
                                <span> | </span>
                                <a href="http://"><span>slides</span></a>
                                <span> | </span>
                                <a href="http://"><span>codes</span></a>
                                <span> | </span>
                                <a href="http://"><span>poster</span></a>
                            </div>
                            <abstract>
                                We propose a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence.
                                We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the
                                embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification, and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding
                                methods in all of the 3 tasks.
                            </abstract>
                        </div>
                    </div>

                    <!-- research 5 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/exp_quant.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Neural networks with few multiplications
                            </h4>
                            <p class="author">
                                Zhouhan Lin, Matthieu Courbariaux, Roland Memisevic, and Yoshua Bengio
                            </p>
                            <div class="link-list">
                                <span class="time">ICLR 2016</span>
                                <span> | </span>
                                <a href="http://"><span>pdf</span></a>
                                <span> | </span>
                                <a href="http://"><span>slides</span></a>
                                <span> | </span>
                                <a href="http://"><span>codes</span></a>
                                <span> | </span>
                                <a href="http://"><span>poster</span></a>
                            </div>
                            <abstract>
                                WFor most deep learning algorithms training is notoriously time consuming. Since most of the computation in training neural networks is typically spent on floating point multiplications, we investigate an approach to training that eliminates the need
                                for most of these. Our method consists of two parts: First we stochastically binarize weights to convert multiplications involved in computing hidden states to sign changes. Second, while back-propagating error derivatives,
                                in addition to binarizing the weights, we quantize the representations at each layer to convert the remaining multiplications into binary shifts. Experimental results across 3 popular datasets (MNIST, CIFAR10, SVHN) show
                                that this approach not only does not hurt classification performance but can result in even better performance than standard stochastic gradient descent training, paving the way to fast, hardwarefriendly training of neural
                                networks.
                            </abstract>
                        </div>
                    </div>

                    <!-- research 6 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/z_lin.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                How far can we go without convolution: Improving fully-connected networks
                            </h4>
                            <p class="author">
                                Zhouhan Lin, Roland Memisevic, and Kishore Konda
                            </p>
                            <div class="link-list">
                                <span class="time">ICLR 2016 workshop</span>
                                <span> | </span>
                                <a href="http://"><span>pdf</span></a>
                                <span> | </span>
                                <a href="http://"><span>slides</span></a>
                                <span> | </span>
                                <a href="http://"><span>codes</span></a>
                                <span> | </span>
                                <a href="http://"><span>poster</span></a>
                            </div>
                            <abstract>
                                We propose ways to improve the performance of fully connected networks. We found that two approaches in particular have a strong effect on performance: linear bottleneck layers and unsupervised pre-training using autoencoders without hidden unit biases.
                                We show how both approaches can be related to improving gradient flow and reducing sparsity in the network. We show that a fully connected network can yield approximately 70% classification accuracy on the permutationinvariant
                                CIFAR-10 task, which is much higher than the current state-of-the-art. By adding deformations to the training data, the fully connected network achieves 78% accuracy, which is just 10% short of a decent convolutional network.
                            </abstract>
                        </div>
                    </div>

                    <!-- research 7 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/rnnternary.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Recurrent Neural Networks with Limited Numerical Precision
                            </h4>
                            <p class="author">
                                Joachim Ott*, Zhouhan Lin*, Ying Zhang, Shih-Chii Liu, and Yoshua Bengio
                            </p>
                            <div class="link-list">
                                <span class="time">ICLR 2016 workshop</span>
                                <span> | </span>
                                <a href="http://"><span>pdf</span></a>
                                <span> | </span>
                                <a href="http://"><span>slides</span></a>
                                <span> | </span>
                                <a href="http://"><span>poster</span></a>
                            </div>
                            <abstract>
                                Recurrent Neural Networks (RNNs) produce state-of-art performance on many machine learning tasks but their demand on resources in terms of memory and computational power are often high. Therefore, there is a great interest in optimizing the computations
                                performed with these models especially when considering development of specialized low-power hardware for deep networks. One way of reducing the computational needs is to limit the numerical precision of the network weights
                                and biases. This has led to different proposed rounding methods which have been applied so far to only Convolutional Neural Networks and Fully-Connected Networks. This paper addresses the question of how to best reduce
                                weight precision during training in the case of RNNs. We present results from the use of different stochastic and deterministic reduced precision training methods applied to three major RNN types which are then tested on
                                several datasets. The results show that the weight binarization methods do not work with the RNNs. However, the stochastic and deterministic ternarization, and pow2-ternarization methods gave rise to low-precision RNNs
                                that produce similar and even higher accuracy on certain datasets therefore providing a path towards training more efficient implementations of RNNs in specialized hardware.
                            </abstract>
                        </div>
                    </div>

                    <!-- research 8 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/hsiclassify.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Deep learning-based classification of hyperspectral data
                            </h4>
                            <p class="author">
                                Yushi Chen, Zhouhan Lin, Xing Zhao, Gang Wang, and Yanfeng Gu
                            </p>
                            <div class="link-list">
                                <span class="time">Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2014</span>
                                <span> | </span>
                                <a href="http://"><span>pdf</span></a>
                                <span> | </span>
                                <a href="http://"><span>codes</span></a>
                            </div>
                            <abstract>
                                Classification is one of the most popular topics in hyperspectral remote sensing. In the last two decades, a huge number of methods were proposed to deal with the hyperspectral data classification problem. However, most of them do not hierarchically extract
                                deep features. In this paper, the concept of deep learning is introduced into hyperspectral data classification for the first time. First, we verify the eligibility of stacked autoencoders by following classical spectral
                                information-based classification. Second, a new way of classifying with spatial-dominated information is proposed. We then propose a novel deep learning framework to merge the two features, from which we can get the highest
                                classification accuracy. The framework is a hybrid of principle component analysis (PCA), deep learning architecture, and logistic regression. Specifically, as a deep learning architecture, stacked autoencoders are aimed
                                to get useful high-level features. Experimental results with widely-used hyperspectral data indicate that classifiers built in this deep learning-based framework provide competitive performance. In addition, the proposed
                                joint spectral–spatial deep neural network opens a new window for future research, showcasing the deep learning-based methods’ huge potential for accurate hyperspectral data classification.
                            </abstract>
                        </div>
                    </div>

                    <!-- 在这里新建一个新的research，可复制一段research代码，从class=row开始, 格式如下 -->
                    <!-- <div class="row">……</div> -->
                </section>

            </section>
            <!-- This is News -->
            <section id="news" class="section-item">
                <h3 style="margin: 0;">
                    News
                </h3>
                <ul class="row">
                    <li>2023/04: One paper is accepted at CogSci 2023.</li>
                    <li>2023/04: One paper is accepted at CogSci 2023.</li>
                    <li>22022/10: New preprint on biologically plausible learning with local activity perturbation.</li>
                    <!-- 在这里新建一条新的news，格式如下 -->
                    <!-- <li>新内容</li> -->
                </ul>
            </section>
        </section>
    </div>
    <!-- jQuery文件。务必在bootstrap.min.js 之前引入 -->
    <script src="https://cdn.staticfile.org/jquery/2.1.1/jquery.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js" crossorigin="anonymous"></script>
    <script src="./index.js"></script>
</body>

</html>