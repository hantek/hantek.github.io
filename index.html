<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ZHOUHAN LIN</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
    <link rel="stylesheet" href="./index.css">
</head>

<body>
    <div>
        <!-- This is header -->
        <header class="home-header">
            <div class="header-content row d-flex flex-wrap align-items-center justify-content-between">
                <div class="header-logo col-xs-12 col-sm-4">ZHOUHAN LIN</div>
                <ul class="col-xs-12 col-sm-8 col-md-auto m-0 d-flex flex-wrap align-items-center justify-content-center justify-content-md-between list-unstyled">

                    <li class="px-4 mx-16">
                        <div class="navbar-header">
                            <a class="navbar-brand" href="#bio">Bio</a>
                        </div>
                    </li>
                    <li class="px-4 mx-16">
                        <div class="navbar-header">
                            <a class="navbar-brand" href="#research">Research</a>
                        </div>
                    </li>
                    <li class="px-4 mx-16">
                        <div class="navbar-header">
                            <a class="navbar-brand" href="#teaching">Teaching</a>
                        </div>
                    </li>
                    <li class="px-4 mx-16">
                        <div class="navbar-header">
                            <a class="navbar-brand" href="#news">News</a>
                        </div>
                    </li>
                    <li class="px-4 mx-16">
                        <div class="navbar-header">
                            <a class="navbar-brand" href="#publications">Publications</a>
                        </div>
                    </li>
                    <!-- 在这里新增header navigation，格式如下： -->
                    <!-- <li class="px-4 mx-16">
                        <div class="navbar-header">
                            <a class="navbar-brand" href="#publications">Publications</a>
                        </div>
                    </li> -->
                </ul>
            </div>
        </header>
        <section class="section-container">
            <section id="head" class="section-item">
                <div class="row">
                    <div class="col-xs-12 col-sm-4">
                        <img src="assets/img/coverphoto.jpg" alt="" srcset="" width="80%">
                    </div>
                    <div class="col-xs-12 col-sm-8">
                        <h2 style="margin-bottom: 24px;">Zhouhan Lin (林洲汉)</h2>
                        <div>
                            <span style="font-weight: 500;">Assistant Professor</span> 
                        </div>
                        <div>
                            <span style="font-weight: 500;"><a href="https://jhc.sjtu.edu.cn/">John Hopcroft Center of Computer Science</a></span> 
                        </div>
                        <div>
                            <span style="font-weight: 500;"><a href="https://en.sjtu.edu.cn/">Shanghai Jiao Tong University</a></span>
                        </div>
                        <div>
                            <span style="font-weight: 500;">E-mail :</span> lastname[dot]firstname[the simbol]gmail.com <span style="font-weight: 500;">or</span> 
                        </div>
                        <p>
                            [github name][the simbol][university abbreviation].edu.cn
                        </p>
                        <p>
                            <span style="font-weight: 500;">Office :</span> Room 437, Dianyuan Building #1
                        </p>
                        <p class="link-block">
                            <a href="https://www.facebook.com/zhouhan.lin.9/" target="_blank">
                                <img src="./assets/facebook.svg" alt="" srcset="">
                            </a>
                            <a href="https://twitter.com/zhouhan_lin" target="_blank">
                                <img src="./assets/twitter.svg" alt="" srcset="">
                            </a>
                            <a href="https://ca.linkedin.com/in/zhouhan-lin-34b98975" target="_blank">
                                <img src="./assets//linkedin.svg" alt="" srcset="">
                            </a>
                            <a href="https://www.zhihu.com/people/linzhouhan" target="_blank">
                                <img src="./assets//zhihu.svg" alt="" srcset="">
                            </a>
                            <a href="https://scholar.google.com/citations?user=LNZ4efwAAAAJ&hl=en" target="_blank">
                                <img src="./assets//googlescholar.svg" alt="" srcset="">
                            </a>
                            <a href="https://www.semanticscholar.org/author/Zhouhan-Lin/3146592" target="_blank">
                                <img src="./assets//semanticscholar.svg" alt="" srcset="">
                            </a>
                            <a href="https://github.com/hantek" target="_blank">
                                <img src="./assets//github.svg" alt="" srcset="">
                            </a>
                            
                        </p>
                    </div>
                </div>
            </section>
            
            <!--This is Bio -->
            <!-- 这里的id="bio"，需要与header中的href="#bio"对应，用来点击滑动定位 -->
            <section id="bio" class="section-item">
                <h3 style="margin: 0;">
                    Bio
                </h3>
                <p>
                    I am an assistant professor at the <a href="https://jhc.sjtu.edu.cn/">John Hopcroft Center of Computer Science</a> at <a href="https://en.sjtu.edu.cn/">Shanghai Jiao Tong University (SJTU)</a>, 
                    and I am leading the <a href="https://github.com/LUMIA-Group">LUMIA group</a>. 
                    Before joining SJTU, I was a visiting scientist at <a href="https://ai.facebook.com/">Facebook AI Research (FAIR)</a> in Menlo Park, CA. 
                    I received my Ph.D. in Computer Science from the <a href="https://mila.quebec/en/">Mila lab</a> in the <a href="https://www.umontreal.ca/en/"> University of Montreal</a> in 2019, 
                    where I was fortunately supervised by <a href="https://mila.quebec/en/">Yoshua Bengio</a>. 
                    Prior to Mila, I received my B.Sc. (2012) and M.Sc. (2014) from Harbin Institute of Technology.  
                </p>
            </section>
            
            <section id="research" class="section-item">
                <h3 style="margin: 0;">
                    Research
                </h3>
                <p>
                    My core research interest is <span style="font-weight: 500;">to explore and develop machine intelligence capable of acquiring, forming, reasoning, and interacting with abstract concepts</span> from large amounts of data. 
                    Almost all of my research efforts are centered around this goal, touching on a diverse range of topics such as 
                    attention mechanisms, language modeling, dialog systems, automated summarization, grammar induction, graph neural networks, code generation, etc. 
                    A list of topics that I am interested in at the moment are:
                </p>
                <ul class="row">
                    <li> Methods that could suppress hallucination in LLMs, such as retrieval-based methods, external memory, etc. </li>
                    <li> Long sequence modeling, such as efficient attention mechanisms, learning multi-scale representations, etc.</li>
                    <li> Modeling highly grammatical and structured sequences, such as SQL and code. </li>
                    <li> Graph representation learning, especially those that include evolving graph structures.</li>
                    <li> Self-supervised learning, especially in a cross-modal setting.</li>
                    <li> Diet dataset for LLMs. </li>
                    <!-- 在这里新建一条新的news，格式如下 -->
                    <!-- <li>新内容</li> -->
                </ul>
            </section>

            <section id="teaching" class="section-item">
                <h3 style="margin: 0;">
                    Teaching
                </h3>
                <ul class="row">
                    <li> CS-1605 Programming Practice (C++) （C++程序设计实践）</li>
                    <li> CS-3602 Natural Language Processing （自然语言处理） </li>
                </ul>
            </section>
            
            <!-- This is News -->
            <section id="news" class="section-item">
                <h3 style="margin: 0;">
                    News
                </h3>
                <ul class="row">
                    <li> May 2023: Two papers (<a href="https://aclanthology.org/2023.acl-long.281/">[1]</a><a href="https://aclanthology.org/2023.findings-acl.570/">[2]</a>) are accepted at ACL 2023! </li>
                    <li> Mar. 2023: <a href="https://scholar.google.com/citations?user=C-TqDNsAAAAJ">Yunchong Song</a> has got the ICLR Travel Award, congratuations! </li>
                    <li> Feb. 2023: Two papers (<a href="https://arxiv.org/abs/2302.09509">[1]</a><a href="https://arxiv.org/abs/2304.05361">[2]</a>) are accepted at ICASSP 2023! </li>
                    <li> Jan. 2023: <a href="https://openreview.net/forum?id=wKPmPBHSnT6">One paper</a> is accepted at ICLR 2023! </li>
                    <li> Oct. 2022: Three papers (<a href="https://aclanthology.org/2022.emnlp-main.211/">[1]</a><a href="https://aclanthology.org/2022.findings-emnlp.173/">[2]</a><a href="https://aclanthology.org/2022.findings-emnlp.114//">[3]</a>) are accepted at EMNLP 2022! </li>
                    <li> Feb. 2022: Two papers (<a href="https://aclanthology.org/2022.acl-long.308/">[1]</a><a href="https://aclanthology.org/2022.acl-long.502/">[2]</a>) are accepted at ACL 2022! </li>
                </ul>
            </section>

            
            <!-- This is publications -->
            <section id="publications" class="section-item">
                <h3 style="margin: 0;">
                    Selected Publications
                </h3>
                <p>
                    <span style="font-weight: 500;">This is a selected list of my publications. For an up-to-date, complete list, please refer to 
                        <a href="https://scholar.google.com/citations?user=LNZ4efwAAAAJ&hl=en">my Google Scholar page</a>. </span>
                </p>
                <section class="research-section">
                    <!-- research 1 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/distanceparser.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator
                            </h4>
                            <p class="author">
                                Ziwei He, Meng Yang, Minwei Feng, Jingcheng Yin, Xinbing Wang, Jingwen Leng, <span style="font-weight: 500;">Zhouhan Lin#</span>
                            </p>
                            <div class="link-list">
                                <span>ACL 2023 (Findings) </span>
                                <span> | </span>
                                <a href="https://aclanthology.org/2023.findings-acl.570.pdf"><span>pdf</span></a>
                                <span> | </span>
                                <a href="http://"><span>slides</span></a>
                                <span> | </span>
                                <a href="https://github.com/LUMIA-Group/FourierTransformer"><span>codes</span></a>
                            </div>
                            <abstract>
                                The transformer model is known to be computationally demanding, and prohibitively costly for long sequences, as the self-attention
                                module uses a quadratic time and space complexity with respect to sequence length. Many researchers have focused on designing new 
                                forms of self-attention or introducing new parameters to overcome this limitation, however a large portion of them prohibits the 
                                model to inherit weights from large pretrained models. In this work, the transformer's inefficiency has been taken care of from 
                                another perspective. We propose Fourier Transformer, a simple yet effective approach by progressively removing redundancies in 
                                hidden sequence using the ready-made Fast Fourier Transform (FFT) operator to perform Discrete Cosine Transformation (DCT). 
                                Fourier Transformer is able to significantly reduce computational costs while retaining the ability to inherit from various 
                                large pretrained models. Experiments show that our model achieves state-of-the-art performances among all transformer-based 
                                models on the long-range modeling benchmark LRA with significant improvement in both speed and space. For generative seq-to-seq 
                                tasks including CNN/DailyMail and ELI5, by inheriting the BART weights our model outperforms the standard BART and other 
                                efficient models.
                        </div>
                    </div>
                    
                    <!-- research 1 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/distanceparser.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Text Classification In The Wild: A Large-Scale Long-Tailed Name Normalization Dataset
                            </h4>
                            <p class="author">
                                Jiexing Qi, Shuhao Li, Zhixin Guo, Yusheng Huang, Chenghu Zhou, Weinan Zhang, Xinbing Wang, <span style="font-weight: 500;">Zhouhan Lin#</span>
                            </p>
                            <div class="link-list">
                                <span>ICASSP 2023 </span>
                                <span> | </span>
                                <a href="https://arxiv.org/pdf/2302.09509.pdf"><span>pdf</span></a>
                                <span> | </span>
                                <a href="http://"><span>slides</span></a>
                                <span> | </span>
                                <a href="https://github.com/LUMIA-Group/LoT-insts"><span>codes</span></a>
                            </div>
                            <abstract>
                                Real-world data usually exhibits a long-tailed distribution, with a few frequent labels and a lot of few-shot labels. 
                                The study of institution name normalization is a perfect application case showing this phenomenon. There are many institutions 
                                worldwide with enormous variations of their names in the publicly available literature. In this work, we first collect a 
                                large-scale institution name normalization dataset LoT-insts1, which contains over 25k classes that exhibit a naturally 
                                long-tailed distribution. In order to isolate the few-shot and zero-shot learning scenarios from the massive many-shot classes,
                                we construct our test set from four different subsets: many-, medium-, and few-shot sets, as well as a zero-shot open set. 
                                We also replicate several important baseline methods on our data, covering a wide range from search-based methods to neural 
                                network methods that use the pretrained BERT model. Further, we propose our specially pretrained, BERT-based model that shows 
                                better out-of-distribution generalization on few-shot and zero-shot test sets. Compared to other datasets focusing on the 
                                long-tailed phenomenon, our dataset has one order of magnitude more training data than the largest existing long-tailed datasets 
                                and is naturally long-tailed rather than manually synthesized. We believe it provides an important and different scenario to 
                                study this problem. To our best knowledge, this is the first natural language dataset that focuses on long-tailed and open-set 
                                classification problems.
                        </div>
                    </div>


                    <!-- research 1 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/distanceparser.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Ordered GNN: Ordering Message Passing to Deal with Heterophily and Over-smoothing
                            </h4>
                            <p class="author">
                                Yunchong Song, Chenghu Zhou, Xinbing Wang, <span style="font-weight: 500;">Zhouhan Lin#</span>
                            </p>
                            <div class="link-list">
                                <span>ICLR 2023 </span>
                                <span> | </span>
                                <a href="https://arxiv.org/pdf/2302.01524.pdf"><span>pdf</span></a>
                                <span> | </span>
                                <a href="http://"><span>slides</span></a>
                                <span> | </span>
                                <a href="https://github.com/LUMIA-Group/OrderedGNN"><span>codes</span></a>
                            </div>
                            <abstract>
                                Most graph neural networks follow the message-passing mechanism. However, it faces the over-smoothing problem when multiple times
                                of message passing is applied to a graph, causing indistinguishable node representations and preventing the model from effectively
                                learning dependencies between farther-away nodes. On the other hand, features of neighboring nodes with different labels are likely 
                                to be falsely mixed, resulting in the heterophily problem. In this work, we propose to order the messages passing into the node 
                                representation, with specific blocks of neurons targeted for message passing within specific hops. This is achieved by aligning 
                                the hierarchy of the rooted-tree of a central node with the ordered neurons in its node representation. Experimental results on 
                                an extensive set of datasets show that our model can simultaneously achieve the state-of-the-art in both homophily and heterophily
                                settings, without any targeted design. Moreover, its performance maintains pretty well while the model becomes really deep, 
                                effectively preventing the over-smoothing problem. Finally, visualizing the gating vectors shows that our model learns to behave 
                                differently between homophily and heterophily settings, providing an explainable graph neural model.
                        </div>
                    </div>

                    <!-- research 1 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/distanceparser.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                RASAT: Integrating Relational Structures into Pretrained Seq2Seq Model for Text-to-SQL
                            </h4>
                            <p class="author">
                                Jiexing Qi, Jingyao Tang, Ziwei He, Xiangpeng Wan, Yu Cheng, Chenghu Zhou, Xinbing Wang, Quanshi Zhang, <span style="font-weight: 500;">Zhouhan Lin#</span>
                            </p>
                            <div class="link-list">
                                <span>ACL 2023 (Findings) </span>
                                <span> | </span>
                                <a href="https://arxiv.org/pdf/2205.06983.pdf"><span>pdf</span></a>
                                <span> | </span>
                                <a href="http://"><span>slides</span></a>
                                <span> | </span>
                                <a href="https://github.com/LUMIA-Group/rasat"><span>codes</span></a>
                            </div>
                            <abstract>
                                Relational structures such as schema linking and schema encoding have been validated as a key component to qualitatively 
                                translating natural language into SQL queries. However, introducing these structural relations comes with prices: they often 
                                result in a specialized model structure, which largely prohibits using large pretrained models in text-to-SQL. To address this 
                                problem, we propose RASAT: a Transformer seq2seq architecture augmented with relation-aware self-attention that could leverage a
                                variety of relational structures while inheriting the pretrained parameters from the T5 model effectively. Our model can 
                                incorporate almost all types of existing relations in the literature, and in addition, we propose introducing co-reference 
                                relations for the multi-turn scenario. Experimental results on three widely used text-to-SQL datasets, covering both single-turn 
                                and multi-turn scenarios, have shown that RASAT could achieve state-of-the-art results across all three benchmarks (75.5% EX on 
                                Spider, 52.6% IEX on SParC, and 37.4% IEX on CoSQL).
                        </div>
                    </div>

                    <!-- research 1 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/distanceparser.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Syntax-guided Localized Self-attention by Constituency Syntactic Distance
                            </h4>
                            <p class="author">
                                Shengyuan Hou*, Jushi Kai*, Haotian Xue*, Bingyu Zhu, Bo Yuan, Longtao Huang, Xinbing Wang, <span style="font-weight: 500;">Zhouhan Lin#</span>
                            </p>
                            <div class="link-list">
                                <span>EMNLP 2022 (Findings) </span>
                                <span> | </span>
                                <a href="https://arxiv.org/pdf/2210.11759.pdf"><span>pdf</span></a>
                                <span> | </span>
                                <a href="http://"><span>slides</span></a>
                                <span> | </span>
                                <a href="https://github.com/LUMIA-Group/distance_transformer"><span>codes</span></a>
                            </div>
                            <abstract>
                                Recent works have revealed that Transformers are implicitly learning the syntactic information in its lower layers from data, 
                                albeit highly dependent on the quality and scale of the training data. However, learning syntactic information from data is 
                                not necessary if we can leverage an external syntactic parser, which provides better parsing quality with well-defined syntactic
                                structures. This could potentially improve the Transformer's performance and sample efficiency. In this work, we propose a 
                                syntax-guided localized self-attention for Transformer that allows directly incorporating grammar structures from an external 
                                constituency parser. It prohibits the attention mechanism from overweight the grammatically distant tokens over close ones. 
                                Experimental results show that our model could consistently improve translation performance on a variety of machine translation
                                datasets, ranging from small to large dataset sizes, and with different source languages.
                        </div>
                    </div>





                    <!-- research 1 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/distanceparser.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Leveraging Unimodal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition
                            </h4>
                            <p class="author">
                                Xichen Pan, Peiyu Chen, Yichen Gong, Helong Zhou, Xinbing Wang, <span style="font-weight: 500;">Zhouhan Lin#</span>
                            </p>
                            <div class="link-list">
                                <span> ACL 2022 </span>
                                <span> | </span>
                                <a href="https://arxiv.org/pdf/2203.07996.pdf"><span>pdf</span></a>
                                <span> | </span>
                                <a href="http://"><span>slides</span></a>
                                <span> | </span>
                                <a href="https://github.com/LUMIA-Group/Leveraging-Self-Supervised-Learning-for-AVSR"><span>codes</span></a>
                            </div>
                            <abstract>
                                Training Transformer-based models demand a large amount of data while obtaining aligned and labeled data in multimodality
                                is rather cost-demanding, especially for audio-visual speech recognition (AVSR). Thus it makes a lot of sense to make use of 
                                unlabelled unimodal data. On the other side, although the effectiveness of large-scale self-supervised learning is well 
                                established in both audio and visual modalities, how to integrate those pre-trained models into a multimodal scenario remains 
                                underexplored. In this work, we successfully leverage unimodal self-supervised learning to promote the multimodal AVSR. In 
                                particular, audio and visual front-ends are trained on large-scale unimodal datasets, and then we integrate components of both 
                                front-ends into a larger multimodal framework that learns to recognize parallel audio-visual data into characters through a 
                                combination of CTC and seq2seq decoding. We show that both components inherited from unimodal self-supervised learning cooperate 
                                well, resulting in the multimodal framework yielding competitive results through fine-tuning. Our model is experimentally 
                                validated on both word-level and sentence-level tasks. Especially, even without an external language model, our proposed model 
                                raises the state-of-the-art performances on the widely accepted Lip Reading Sentences 2 (LRS2) dataset by a large margin, with 
                                a relative improvement of 30%.
                        </div>
                    </div>


                    <!-- research 1 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/distanceparser.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Block-Skim: Efficient Question Answering for Transformer
                            </h4>
                            <p class="author">
                                Yue Guan, Zhengyi Li, Jingwen Leng#, <span style="font-weight: 500;">Zhouhan Lin#</span>, Minyi Guo, Yuhao Zhu
                            </p>
                            <div class="link-list">
                                <span> AAAI 2022 </span>
                                <span> | </span>
                                <a href="https://arxiv.org/pdf/2112.08560.pdf"><span>pdf</span></a>
                                <span> | </span>
                                <a href="http://"><span>slides</span></a>
                                <span> | </span>
                                <a href="https://"><span>codes</span></a>
                            </div>
                            <abstract>
                                Transformer models have achieved promising results on natural language processing (NLP) tasks including extractive question 
                                answering (QA). Common Transformer encoders used in NLP tasks process the hidden states of all input tokens in the context 
                                paragraph throughout all layers. However, different from other tasks such as sequence classification, answering the raised
                                question does not necessarily need all the tokens in the context paragraph. Following this motivation, we propose Block-skim,
                                which learns to skim unnecessary context in higher hidden layers to improve and accelerate the Transformer performance. The key 
                                idea of Block-Skim is to identify the context that must be further processed and those that could be safely discarded early on 
                                during inference. Critically, we find that such information could be sufficiently derived from the self-attention weights inside 
                                the Transformer model. We further prune the hidden states corresponding to the unnecessary positions early in lower layers, 
                                achieving significant inference-time speedup. To our surprise, we observe that models pruned in this way outperform their 
                                full-size counterparts. Block-Skim improves QA models' accuracy on different datasets and achieves 3 times speedup on BERT-base 
                                model.
                        </div>
                    </div>

                    <!-- research 1 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/distanceparser.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Exploiting Syntactic Structure for Better Language Modeling: A Syntactic Distance Approach
                            </h4>
                            <p class="author">
                                Wenyu Du*,  <span style="font-weight: 500;">Zhouhan Lin</span>*, Yikang Shen, Timothy J. O'Donnell, Yoshua Bengio, Yue Zhang#
                            </p>
                            <div class="link-list">
                                <span> ACL 2020 </span>
                                <span> | </span>
                                <a href="https://arxiv.org/pdf/2005.05864.pdf"><span>pdf</span></a>
                                <span> | </span>
                                <a href="http://"><span>slides</span></a>
                                <span> | </span>
                                <a href="https://"><span>codes</span></a>
                            </div>
                            <abstract>
                                It is commonly believed that knowledge of syntactic structure should improve language modeling. However, effectively and 
                                computationally efficiently incorporating syntactic structure into neural language models has been a challenging topic. In this paper,
                                we make use of a multi-task objective, i.e., the models simultaneously predict words as well as ground truth parse trees in a form 
                                called "syntactic distances", where information between these two separate objectives shares the same intermediate representation. 
                                Experimental results on the Penn Treebank and Chinese Treebank datasets show that when ground truth parse trees are provided as 
                                additional training signals, the model is able to achieve lower perplexity and induce trees with better quality.
                        </div>
                    </div>
                    
                    
                    
                    <!-- research 1 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/distanceparser.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Straight to the Tree: Constituency Parsing with Neural Syntactic Distance
                            </h4>
                            <p class="author">
                                Yikang Shen*, <span style="font-weight: 500;">Zhouhan Lin</span>*, Athul Paul Jacob, Alessandro Sordoni, Aaron Courville, Yoshua Bengio
                            </p>
                            <div class="link-list">
                                <span>ACL 2018 </span>
                                <span> | </span>
                                <a href="http://"><span>pdf</span></a>
                                <span> | </span>
                                <a href="http://"><span>slides</span></a>
                                <span> | </span>
                                <a href="http://"><span>codes</span></a>
                            </div>
                            <abstract>
                                We propose a novel constituency parsing scheme. The model predicts a vector of real-valued scalars, named syntactic distances, for each split position in the input sentence. The syntactic distances specify the order in which the split points will be selected,
                                recursively partitioning the input, in a top-down fashion. Compared to traditional shiftreduce parsing schemes, our approach is free from the potential problem of compounding errors, while being faster and easier to parallelize.
                                Our model achieves competitive performance amongst single model, discriminative parsers in the PTB dataset and outperforms previous models in the CTB dataset.
                            </abstract>
                        </div>
                    </div>

                    <!-- research 2 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/rrnn.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Learning Hierarchical Structures On-The-Fly with a Recurrent-Recursive Model for Sequences
                            </h4>
                            <p class="author">
                                Athul Paul Jacob*, <span style="font-weight: 500;">Zhouhan Lin</span>*, Alessandro Sordoni, Yoshua Bengio
                            </p>
                            <div class="link-list">
                                <span>ACL 2018 workshop</span>
                                <span> | </span>
                                <a href="http://"><span>pdf</span></a>
                                <span> | </span>
                                <a href="http://"><span>poster</span></a>
                            </div>
                            <abstract>
                                We propose a hierarchical model for sequential data that learns a tree on-the-fly, i.e. while reading the sequence. In the model, a recurrent network adapts its structure and reuses recurrent weights in a recursive manner. This creates adaptive skip-connections
                                that ease the learning of long-term dependencies. The tree structure can either be inferred without supervision through reinforcement learning, or learned in a supervised manner. We provide preliminary experiments in a
                                novel Math Expression Evaluation (MEE) task, which is explicitly crafted to have a hierarchical tree structure that can be used to study the effectiveness of our model. Additionally, we test our model in a wellknown propositional
                                logic and language modelling tasks. Experimental results show the potential of our approach.
                            </abstract>
                        </div>
                    </div>
                    <!-- research 3 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/tree.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Neural Language Modeling by Jointly Learning Syntax and Lexicon
                            </h4>
                            <p class="author">
                                Yikang Shen, <span style="font-weight: 500;">Zhouhan Lin</span>, Chin-Wei Huang, Aaron Courville
                            </p>
                            <div class="link-list">
                                <span>ICLR 2018</span>
                                <span> | </span>
                                <a href="http://"><span>pdf</span></a>
                                <span> | </span>
                                <a href="http://"><span>slides</span></a>
                                <span> | </span>
                                <a href="http://"><span>codes</span></a>
                                <span> | </span>
                                <a href="http://"><span>poster</span></a>
                            </div>
                            <abstract>
                                We propose a neural language model capable of unsupervised syntactic structure induction. The model leverages the structure information to form better semantic representations and better language modeling. Standard recurrent neural networks are limited
                                by their structure and fail to efficiently use syntactic information. On the other hand, tree-structured recursive networks usually require additional structural supervision at the cost of human expert annotation. In this
                                paper, We propose a novel neural language model, called the Parsing-Reading-Predict Networks (PRPN), that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure
                                to learn a better language model. In our model, the gradient can be directly back-propagated from the language model loss into the neural parsing network. Experiments show that the proposed model can discover the underlying
                                syntactic structure and achieve state-of-the-art performance on word/character-level language model tasks.
                            </abstract>
                        </div>
                    </div>
                    <!-- research 4 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/semlp.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                A structured self-attentive Sentence Embedding
                            </h4>
                            <p class="author">
                                <span style="font-weight: 500;">Zhouhan Lin</span>, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou and Yoshua Bengio
                            </p>
                            <div class="link-list">
                                <span class="time">ICLR 2017</span>
                                <span> | </span>
                                <a href="http://"><span>pdf</span></a>
                                <span> | </span>
                                <a href="http://"><span>slides</span></a>
                                <span> | </span>
                                <a href="http://"><span>codes</span></a>
                                <span> | </span>
                                <a href="http://"><span>poster</span></a>
                            </div>
                            <abstract>
                                We propose a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence.
                                We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the
                                embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification, and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding
                                methods in all of the 3 tasks.
                            </abstract>
                        </div>
                    </div>

                    <!-- research 5 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/exp_quant.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Neural networks with few multiplications
                            </h4>
                            <p class="author">
                                <span style="font-weight: 500;">Zhouhan Lin</span>, Matthieu Courbariaux, Roland Memisevic, and Yoshua Bengio
                            </p>
                            <div class="link-list">
                                <span class="time">ICLR 2016 ()oral</span>
                                <span> | </span>
                                <a href="http://"><span>pdf</span></a>
                                <span> | </span>
                                <a href="http://"><span>slides</span></a>
                                <span> | </span>
                                <a href="http://"><span>codes</span></a>
                                <span> | </span>
                                <a href="http://"><span>poster</span></a>
                            </div>
                            <abstract>
                                WFor most deep learning algorithms training is notoriously time consuming. Since most of the computation in training neural networks is typically spent on floating point multiplications, we investigate an approach to training that eliminates the need
                                for most of these. Our method consists of two parts: First we stochastically binarize weights to convert multiplications involved in computing hidden states to sign changes. Second, while back-propagating error derivatives,
                                in addition to binarizing the weights, we quantize the representations at each layer to convert the remaining multiplications into binary shifts. Experimental results across 3 popular datasets (MNIST, CIFAR10, SVHN) show
                                that this approach not only does not hurt classification performance but can result in even better performance than standard stochastic gradient descent training, paving the way to fast, hardwarefriendly training of neural
                                networks.
                            </abstract>
                        </div>
                    </div>

                    <!-- research 6 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/z_lin.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                How far can we go without convolution: Improving fully-connected networks
                            </h4>
                            <p class="author">
                                <span style="font-weight: 500;">Zhouhan Lin</span>, Roland Memisevic, and Kishore Konda
                            </p>
                            <div class="link-list">
                                <span class="time">ICLR 2016 workshop</span>
                                <span> | </span>
                                <a href="http://"><span>pdf</span></a>
                                <span> | </span>
                                <a href="http://"><span>slides</span></a>
                                <span> | </span>
                                <a href="http://"><span>codes</span></a>
                                <span> | </span>
                                <a href="http://"><span>poster</span></a>
                            </div>
                            <abstract>
                                We propose ways to improve the performance of fully connected networks. We found that two approaches in particular have a strong effect on performance: linear bottleneck layers and unsupervised pre-training using autoencoders without hidden unit biases.
                                We show how both approaches can be related to improving gradient flow and reducing sparsity in the network. We show that a fully connected network can yield approximately 70% classification accuracy on the permutationinvariant
                                CIFAR-10 task, which is much higher than the current state-of-the-art. By adding deformations to the training data, the fully connected network achieves 78% accuracy, which is just 10% short of a decent convolutional network.
                            </abstract>
                        </div>
                    </div>

                    <!-- research 7 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/rnnternary.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Recurrent Neural Networks with Limited Numerical Precision
                            </h4>
                            <p class="author">
                                Joachim Ott*, <span style="font-weight: 500;">Zhouhan Lin</span>*, Ying Zhang, Shih-Chii Liu, and Yoshua Bengio
                            </p>
                            <div class="link-list">
                                <span class="time">ICLR 2016 workshop</span>
                                <span> | </span>
                                <a href="http://"><span>pdf</span></a>
                                <span> | </span>
                                <a href="http://"><span>slides</span></a>
                                <span> | </span>
                                <a href="http://"><span>poster</span></a>
                            </div>
                            <abstract>
                                Recurrent Neural Networks (RNNs) produce state-of-art performance on many machine learning tasks but their demand on resources in terms of memory and computational power are often high. Therefore, there is a great interest in optimizing the computations
                                performed with these models especially when considering development of specialized low-power hardware for deep networks. One way of reducing the computational needs is to limit the numerical precision of the network weights
                                and biases. This has led to different proposed rounding methods which have been applied so far to only Convolutional Neural Networks and Fully-Connected Networks. This paper addresses the question of how to best reduce
                                weight precision during training in the case of RNNs. We present results from the use of different stochastic and deterministic reduced precision training methods applied to three major RNN types which are then tested on
                                several datasets. The results show that the weight binarization methods do not work with the RNNs. However, the stochastic and deterministic ternarization, and pow2-ternarization methods gave rise to low-precision RNNs
                                that produce similar and even higher accuracy on certain datasets therefore providing a path towards training more efficient implementations of RNNs in specialized hardware.
                            </abstract>
                        </div>
                    </div>

                    <!-- research 8 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/hsiclassify.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Deep learning-based classification of hyperspectral data
                            </h4>
                            <p class="author">
                                Yushi Chen, <span style="font-weight: 500;">Zhouhan Lin</span>, Xing Zhao, Gang Wang, and Yanfeng Gu
                            </p>
                            <div class="link-list">
                                <span class="time">Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2014</span>
                                <span> | </span>
                                <a href="http://"><span>pdf</span></a>
                                <span> | </span>
                                <a href="http://"><span>codes</span></a>
                            </div>
                            <abstract>
                                Classification is one of the most popular topics in hyperspectral remote sensing. In the last two decades, a huge number of methods were proposed to deal with the hyperspectral data classification problem. However, most of them do not hierarchically extract
                                deep features. In this paper, the concept of deep learning is introduced into hyperspectral data classification for the first time. First, we verify the eligibility of stacked autoencoders by following classical spectral
                                information-based classification. Second, a new way of classifying with spatial-dominated information is proposed. We then propose a novel deep learning framework to merge the two features, from which we can get the highest
                                classification accuracy. The framework is a hybrid of principle component analysis (PCA), deep learning architecture, and logistic regression. Specifically, as a deep learning architecture, stacked autoencoders are aimed
                                to get useful high-level features. Experimental results with widely-used hyperspectral data indicate that classifiers built in this deep learning-based framework provide competitive performance. In addition, the proposed
                                joint spectral–spatial deep neural network opens a new window for future research, showcasing the deep learning-based methods’ huge potential for accurate hyperspectral data classification.
                            </abstract>
                        </div>
                    </div>

                    <!-- 在这里新建一个新的research，可复制一段research代码，从class=row开始, 格式如下 -->
                    <!-- <div class="row">……</div> -->
                </section>

            </section>

        </section>
    </div>
    <!-- jQuery文件。务必在bootstrap.min.js 之前引入 -->
    <script src="https://cdn.staticfile.org/jquery/2.1.1/jquery.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js" crossorigin="anonymous"></script>
    <script src="./index.js"></script>
</body>

</html>
