<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ZHOUHAN LIN</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
    <link rel="stylesheet" href="./index.css">
</head>

<body>
    <div>
        <!-- This is header -->
        <header class="home-header">
            <div class="header-content row d-flex flex-wrap align-items-center justify-content-between">
                <div class="header-logo col-sm-4">ZHOUHAN LIN</div>
                <ul class="col-sm-8 col-md-auto m-0 d-flex flex-wrap align-items-center justify-content-center justify-content-md-between list-unstyled">

                    <li class="px-4 mx-16">
                        <div class="navbar-header">
                            <a class="navbar-brand" href="#bio">Bio</a>
                        </div>
                    </li>
                    <li class="px-4 mx-16">
                        <div class="navbar-header">
                            <a class="navbar-brand" href="#research">Research</a>
                        </div>
                    </li>
                    <li class="px-4 mx-16">
                        <div class="navbar-header">
                            <a class="navbar-brand" href="#teaching">Teaching</a>
                        </div>
                    </li>
                    <li class="px-4 mx-16">
                        <div class="navbar-header">
                            <a class="navbar-brand" href="#news">News</a>
                        </div>
                    </li>
                    <li class="px-4 mx-16">
                        <div class="navbar-header">
                            <a class="navbar-brand" href="#publications">Publications</a>
                        </div>
                    </li>
                    <!-- 在这里新增header navigation，格式如下： -->
                    <!-- <li class="px-4 mx-16">
                        <div class="navbar-header">
                            <a class="navbar-brand" href="#publications">Publications</a>
                        </div>
                    </li> -->
                </ul>
            </div>
        </header>
        <section class="section-container">
            <section id="head" class="section-item">
                <div class="row">
                    <div class="col-xs-12 col-sm-4">
                        <img src="assets/img/coverphoto.jpg" alt="" srcset="" width="80%">
                    </div>
                    <div class="col-xs-12 col-sm-8">
                        <h2 style="margin-bottom: 24px;">Zhouhan Lin (林洲汉)</h2>
                        <div>
                            <span style="font-weight: 500; font-size: 24px;">Associate Professor</span>
                        </div>
                        <div>
                            <span style="font-weight: 500;">Deputy Director of <a href="https://jhc.sjtu.edu.cn/">John Hopcroft Center of Computer Science (JHC)</a></span>
                        </div>
                        <div>
                            <span style="font-weight: 500;"><a href="https://en.sjtu.edu.cn/">Shanghai Jiao Tong University</a></span>
                        </div>
                        <div style="display: flex;">
                            <div style="margin-right: 4px;">
                                <span style="font-weight: 500;">E-mail : 
                            </div>
                            <div>
                                <span> lastname[dot]firstname[the simbol]gmail.com <span style="font-weight: 500;">or</span></span>
                                <p>[github name][the simbol][university abbreviation].edu.cn</p>
                            </div>
                        </div>

                        <p>
                            <span style="font-weight: 500;">Office :</span> Room 815B, Neo Bay #1 Building South Tower, No. 951 Jianchuan Road.
                        </p>
                        <p class="link-block">
                            <a href="https://scholar.google.com/citations?user=LNZ4efwAAAAJ&hl=en" target="_blank">
                                <img src="./assets//gscholar.png" alt="" srcset="" style="width: 32px;">
                            </a>
                            <a href="https://www.semanticscholar.org/author/Zhouhan-Lin/3146592" target="_blank">
                                <img src="./assets/sscholar.png" alt="" srcset="">
                            </a>
                            <a href="https://github.com/hantek" target="_blank">
                                <img src="./assets/github.svg" alt="" srcset="">
                            </a>
                            <a href="https://www.zhihu.com/people/linzhouhan" target="_blank">
                                <img src="./assets/zhihu.svg" alt="" srcset="">
                            </a>
                            <a href="https://www.facebook.com/zhouhan.lin.9/" target="_blank">
                                <img src="./assets/facebook.svg" alt="" srcset="" style="width: 32px;">
                            </a>
                            <a href="https://twitter.com/zhouhan_lin" target="_blank">
                                <img src="./assets/twitter.svg" alt="" srcset="">
                            </a>
                            <a href="https://ca.linkedin.com/in/zhouhan-lin-34b98975" target="_blank">
                                <img src="./assets/linkedin.svg" alt="" srcset="">
                            </a>
                        </p>
                    </div>
                </div>
            </section>

            <!--This is Bio -->
            <!-- 这里的id="bio"，需要与header中的href="#bio"对应，用来点击滑动定位 -->
            <section id="bio" class="section-item">
                <h3 style="margin: 0;">
                    Bio
                </h3>
                <p>
                    I am an associate professor at the <a href="https://sai.sjtu.edu.cn/">School of Artificial Intellengence</a> and the 
                    <a href="https://jhc.sjtu.edu.cn/">John Hopcroft Center of Computer Science</a> 
                    at <a href="https://en.sjtu.edu.cn/">Shanghai Jiao Tong University (SJTU)</a>.
                    I am leading the <a href="https://github.com/LUMIA-Group">Language Understanding and Machine Intelligence Algorithms (LUMIA) Lab</a>.
                    Before joining SJTU, I was a visiting scientist at <a href="https://ai.facebook.com/">Facebook AI Research (FAIR)</a> in Menlo Park, CA, 
                    working with <a href="https://michaelauli.github.io/">Michael Auli</a>. 
                    I received my Ph.D. in Computer Science from the <a href="https://mila.quebec/en/">Mila lab</a> 
                    in the <a href="https://www.umontreal.ca/en/"> University of Montreal</a> in 2019, 
                    where I was fortunately supervised by <a href="https://yoshuabengio.org/">Yoshua Bengio</a>. 
                    During my Ph.D., I've been interning at
                    <a href="https://research.google/teams/language/">Google AI Language team</a> in the New York City, and at <a href="https://www.ibm.com/watson">IBM Watson</a> 
                    with <a href="https://scholar.google.com/citations?user=h3Nsz6YAAAAJ&hl=en">Bowen Zhou</a> and 
                    <a href="https://sites.google.com/site/moyunlp/">Mo Yu</a> in Yorktown Height, NY. I also worked as a part-time student researcher at 
                    <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-montreal/">Microsoft Research</a>
                    with <a href="https://www.microsoft.com/en-us/research/people/alsordon/">Alessandro Sordoni</a> and 
                    <a href="https://scholar.google.ca/citations?hl=en">Adam Trischler</a> in Montreal.
                    Prior to Mila, I received my B.Sc. (2012) and M.Sc. (2014) degrees from 
                    <a href="http://en.hit.edu.cn/">Harbin Institute of Technology</a>. For more information, you can find my CV <a href="assets/CV_Zhouhan_Lin.pdf">here</a>. 
                </p>
                <p>
                    I am actively looking for highly motivated undergrads, interns, and <span style="font-weight: 500;">prospective Master's (2027) or Ph.D. (2027) students</span> to work with me. 
                    If you are interested in my research topics, please kindly drop me an email, preferably the @sjtu.edu.cn address.
                    <span style="font-weight: 500;">Unfortunately, I have no more Ph.D. nor Master positions available for the year 2026.</span>
                </p>

            </section>

            <section id="research" class="section-item">
                <h3 style="margin: 0;">
                    Research
                </h3>
                <p>
                    My core research interest is <span style="font-weight: 500;">to explore and develop machine intelligence capable of acquiring, forming, reasoning, and interacting with abstract concepts</span> from large amounts of data，
                    especially through self-supervised learning. 
                    Almost all of my research efforts are centered around this goal, touching on a diverse range of topics such as attention mechanisms, language modeling, dialog systems, automated summarization, grammar induction, 
                    graph neural networks, code generation, etc. A list of topics that I am interested in at the moment are:
                </p>
                <ul class="row">
                    <li> New neural network architectures beyond decoder-only Transformers, such as learning multi-scale representations, external memory, etc. </li>
                    <li> New pretraining tasks beyond next-token-prediction, such as retriever immitation, next-context-prediction, etc. </li>
                    <li> Long sequence modeling and efficient models, such as efficient attention mechanisms, KV compression, etc.</li>
                    <!-- 在这里新建一条新的news，格式如下 -->
                    <!-- <li>新内容</li> -->
                </ul>
            </section>

            <section id="teaching" class="section-item">
                <h3 style="margin: 0;">
                    Teaching
                </h3>
                <ul class="row">
                    <li> CS-1605 Programming Practice (C++) （C++程序设计实践）</li>
                    <li> CS-3602 Natural Language Processing （自然语言处理） </li>
                </ul>
            </section>

            <!-- This is News -->
            <section id="news" class="section-item">
                <h3 style="margin: 0;">
                    News
                </h3>
                <ul class="row">
                    <li> Dec 2025: Our LUMIA lab members are going to present our Memory Decoder (<a href="https://arxiv.org/abs/2508.09874">https://arxiv.org/abs/2508.09874</a>) in NeurIPS! An external memory to LLM that allows you to plug-and-play domain knowledge! </li>
                    <li> Nov. 2025: I am serving as the organization chair of <a href="https://mlnlp.org/mlnlp2025/">MLNLP 2025</a>.  </li>
                    <li> Nov. 2025: I gave a talk at NYU Shanghai: "New Architectures for LLMs: Preliminary Explorations and Insights" (in English).  </li>
                    <li> Nov. 2025: I am organizing the workshop on "Efficient LLM architectures" in <a href="http://lmg.cipsc.org.cn/conference/lmg2025/subForum/subForum4/index.html">LMG 2025</a>. </li>
                    <li> Aug. 2025: I am serving as the workshop chair for <a href="http://cips-cl.org/static/CCL2025/cclCommittee/comChairs/index.html">CCL 2025</a>. </li>
                    <li> Aug. 2025: I gave an invited keynote talk on <a href="https://c2sml.cn/conference.html">CSML 2025</a>, "New Architectures for LLMs: Preliminary Explorations and Insights" (in Chinese).  </li>
                    <li> Dec. 2024: I gave two talks on <a href="http://lmg.cipsc.org.cn/conference/cips-lmg2024/index.html">CIPS-LMG 2024</a>. The slides can be found <a href="assets/HuRef LMG演讲.pdf">here</a> (our NeurIPS work on human-readable fingerprint) and <a href="assets/LMG讲习班-1.5h.pdf">here</a> (the efficient LLM tutorial). </li>
                    <li> Sep. 2024: Our human-readable LLM fingerprint (<a href="https://arxiv.org/abs/2312.04828">[1]</a>) and Cluster-wise Graph Transformer <a href="https://openreview.net/pdf?id=3j2nasmKkP">[2]</a>) got accepted at NeurIPS 2024, with [2] being spotlight! </li>
                    <li> Sep. 2024: Four papers (<a href="https://openreview.net/pdf?id=KpR53ZqtQ6">[1]</a><a href="https://openreview.net/pdf?id=T7fzvcq7Mf">[2]</a><a href="https://openreview.net/pdf?id=uJB6HtUAHX">[3]</a><a href="https://openreview.net/pdf?id=2shnXVWH6c">[4]</a>) are accepted
                         at EMNLP 2024! </li>
                    <li> Jan. 2024: <a href="https://openreview.net/forum?id=hv3SklibkL">One paper</a> got accepted at ICLR 2024! </li>
                    <li> Jan. 2024: Our pretrained geoscience LLMs are fully released! Papers, codes and checkpoints are available! The 30B model, named <a href="https://arxiv.org/pdf/2401.00434">GeoGalactica</a> (<a href="https://github.com/geobrain-ai/geogalactica">code&checkpoints</a>), 
                         is based on Galactica, and the 7B model, named <a href="https://dl.acm.org/doi/abs/10.1145/3616855.3635772">K2</a> (<a href="https://github.com/davendw49/k2">code&checkpoints</a>),  is based on LLaMA. </li>
                    
                    <!--These areoutdated activities, commenting out
                    <li> Sep. 2023: <a href="https://openreview.net/pdf?id=t2hEZadBBk">One paper</a> got accepted at NeurIPS 2023! </li>
                    <li> May. 2023: Two papers (<a href="https://aclanthology.org/2023.acl-long.281/">[1]</a><a href="https://aclanthology.org/2023.findings-acl.570/">[2]</a>) are accepted at ACL 2023! </li>
                    <li> Mar. 2023: <a href="https://scholar.google.com/citations?user=C-TqDNsAAAAJ">Yunchong Song</a> has got the ICLR Travel Award, congratuations! </li>
                    <li> Feb. 2023: Two papers (<a href="https://arxiv.org/abs/2302.09509">[1]</a><a href="https://arxiv.org/abs/2304.05361">[2]</a>) are accepted at ICASSP 2023! </li>
                    <li> Jan. 2023: <a href="https://openreview.net/forum?id=wKPmPBHSnT6">One paper</a> is accepted at ICLR 2023! </li>
                    <li> Oct. 2022: Three papers (<a href="https://aclanthology.org/2022.emnlp-main.211/">[1]</a><a href="https://aclanthology.org/2022.findings-emnlp.173/">[2]</a><a href="https://aclanthology.org/2022.findings-emnlp.114//">[3]</a>) are accepted
                        at EMNLP 2022! </li>
                    <li> Feb. 2022: Two papers (<a href="https://aclanthology.org/2022.acl-long.308/">[1]</a><a href="https://aclanthology.org/2022.acl-long.502/">[2]</a>) are accepted at ACL 2022! </li>
                    -->
                </ul>
            </section>


            <!-- This is publications -->
            <section id="publications" class="section-item">
                <h3 style="margin: 0;">
                    Selected Publications
                </h3>
                <p>
                    <span style="font-weight: 500;">This is a selected list of my publications. For an up-to-date, complete list, please refer to 
                        <a href="https://scholar.google.com/citations?user=LNZ4efwAAAAJ&hl=en">my Google Scholar page</a>. </span>
                </p>
                <p>
                    <span style="font-weight: 500;">(*) indicates equal contribution. (#) indicates the corresponding author. </span>
                </p>

                <section class="research-section">
                    <!-- Memory Decoder -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/memdec-intro.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models
                            </h4>
                            <p class="author">
                                Jiaqi Cao*, Jiarui Wang*, Rubin Wei, Qipeng Guo, Kai Chen, Bowen Zhou, <span style="font-weight: 500;">Zhouhan Lin#</span>
                            </p>
                            <div class="link-list">
                                <span>NeurIPS 2025 (Poster) </span>
                                <span> | </span>
                                <a href="https://www.arxiv.org/pdf/2508.09874"><span>pdf</span></a>
                                <span> | </span>
                                <a href="https://github.com/LUMIA-Group/MemoryDecoder/tree/main"><span>codes</span></a>
                                <span> | </span>
                                <a href="https://huggingface.co/collections/Clover-Hill/memorydecoder"><span>checkpoints</span></a>
                                <span> | </span>
                                <a href="https://www.qbitai.com/2025/08/323458.html"><span>量子位</span></a>
                                <span> | </span>
                                <a href="https://www.youtube.com/watch?v=-oV0HpUpoGs"><span>youtube</span></a>
                                <span> | </span>
                                <a href="https://www.bilibili.com/video/BV1uGkZBUE1F"><span>bilibili</span></a>
                            </div>
                            <abstract>
                                We propose Memory Decoder, a plug-and-play pretrained memory that enables efficient domain adaptation without changing the original model’s parameters. Memory Decoder employs a small transformer decoder that learns to 
                                imitate the behavior of an external non-parametric retriever. Once trained, Memory Decoder can be seamlessly integrated with any pretrained language model that shares the same tokenizer, requiring no model-specific modifications.
                                Experimental results demonstrate that Memory Decoder enables effective adaptation of various Qwen and Llama models to three distinct specialized domains: biomedicine, finance, and law, reducing perplexity by an average of 6.17 points. 
                                Overall, Memory Decoder introduces a novel paradigm centered on a specially pretrained memory component designed for domain-specific adaptation. This memory architecture can be integrated in a plug-and-play manner, 
                                consistently enhancing performance across multiple models within the target domain.
                        </div>
                    </div>
                    
                    <!-- research 1 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/gpn.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Graph Parsing Networks
                            </h4>
                            <p class="author">
                                Yunchong Song, Siyuan Huang, Xinbing Wang, Chenghu Zhou, <span style="font-weight: 500;">Zhouhan Lin#</span>
                            </p>
                            <div class="link-list">
                                <span>ICLR 2024 (Poster) </span>
                                <span> | </span>
                                <a href="https://arxiv.org/pdf/2402.14393"><span>pdf</span></a>
                                <span> | </span>
                                <a href="https://github.com/LUMIA-Group/GraphParsingNetworks"><span>codes</span></a>
                            </div>
                            <abstract>
                                Graph pooling compresses graph information into a compact representation. State-of-the-art graph pooling methods follow a hierarchical approach, which reduces the graph size step-by-step. These methods must balance memory efficiency with preserving node information, depending on whether they use node dropping or node clustering. Additionally, fixed pooling ratios or numbers of pooling layers are predefined for all graphs, which prevents personalized pooling structures from being captured for each individual graph. In this work, inspired by bottom-up grammar induction, we propose an efficient graph parsing algorithm to infer the pooling structure, which then drives graph pooling. The resulting Graph Parsing Network (GPN) adaptively learns personalized pooling structure for each individual graph. GPN benefits from the discrete assignments generated by the graph parsing algorithm, allowing good memory efficiency while preserving node information intact. Experimental results on standard benchmarks demonstrate that GPN outperforms state-of-the-art graph pooling methods in graph classification tasks while being able to achieve competitive performance in node classification tasks. We also conduct a graph reconstruction task to show GPN's ability to preserve node information and measure both memory and time efficiency through relevant tests.
                    </div>
                    
                    <!-- research 1 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/muteswap.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Visual-informed Silent Video Identity Conversion
                            </h4>
                            <p class="author">
                                Yifan Liu, Yu Fang, <span style="font-weight: 500;">Zhouhan Lin#</span>
                            </p>
                            <div class="link-list">
                                <span>ACM MultiMedia 25 </span>
                                <span> | </span>
                                <a href="https://www.arxiv.org/pdf/2507.00498"><span>pdf</span></a>
                                <span> | </span>
                                <a href="https://pussycat0700.github.io/MuteSwap-Demo/"><span>demo page</span></a>
                                <span> | </span>
                                <a href="https://dl.acm.org/doi/suppl/10.1145/3746027.3755678/suppl_file/mfp5195-promo-video.mp4"><span>video</span></a>
                                <span> | </span>
                                <a href="https://github.com/PussyCat0700/DiVISe"><span>codes</span></a>
                            </div>
                            <abstract>
                                Conventional voice conversion modifies voice characteristics from a source speaker to a target speaker, relying on audio input from both sides. However, this process becomes infeasible when clean audio is unavailable, such as in silent videos or noisy environments. In this work, we focus on the task of Silent Face-based Voice Conversion (SFVC), which does voice conversion entirely from visual inputs. i.e., given images of a target speaker and a silent video of a source speaker containing lip motion, SFVC generates speech aligning the identity of the target speaker while preserving the speech content in the source silent video. As this task requires generating intelligible speech and converting identity using only visual cues, it is particularly challenging. To address this, we introduce MuteSwap, a novel framework that employs contrastive learning to align cross-modality identities and minimize mutual information to separate shared visual features. Experimental results show that MuteSwap achieves impressive performance in both speech synthesis and identity conversion, especially under noisy conditions where methods dependent on audio input fail to produce intelligible results, demonstrating both the effectiveness of our training approach and the feasibility of SFVC.
                    </div>

                    <!-- research 1 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/anchor.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Training LLMs to be Better Text Embedders through Bidirectional Reconstruction
                            </h4>
                            <p class="author">
                                Chang Su, Dengliang Shi, Siyuan Huang, Jintao Du, Changhua Meng, Yu Cheng, Weiqiang Wang, <span style="font-weight: 500;">Zhouhan Lin#</span>
                            </p>
                            <div class="link-list">
                                <span>EMNLP 2025 (Main Conference) </span>
                                <span> | </span>
                                <a href="https://aclanthology.org/2025.emnlp-main.216.pdf"><span>pdf</span></a>
                                <span> | </span>
                                <a href="https://github.com/LUMIA-Group/Anchor-Embedding"><span>codes</span></a>
                            </div>
                            <abstract>
                                We propose a simple yet effective approach to enhance the semantic representation of the final token embedding in large language models (LLMs) for text embedding tasks. Existing LLM-based embedding methods often rely on the embedding of a reserved token such as [EOS],
                                which is not explicitly trained to capture the full context semantics, limiting its effectiveness for retrieval and re-ranking. Our method introduces an additional training stage prior to contrastive learning, employing 
                                bidirectional generative reconstruction tasks—EBQ2D (Embedding-Based Query-to-Document) and EBD2Q (Embedding-Based Document-to-Query)—to anchor the [EOS] embedding and reconstruct both sides of query-document pairs. 
                                Experimental results show that our approach significantly improves LLM performance on the Massive Text Embedding Benchmark (MTEB), achieving new state-of-the-art results across multiple LLM base models and scales.
                        </div>
                    </div>

                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/reranker.png" alt="Diagram showing the Gumbel Reranking process with a differentiable attention mask over candidate documents." srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Gumbel Reranking: Differentiable End-to-End Reranker Optimization
                            </h4>
                            <p class="author">
                                Siyuan Huang, Zhiyuan Ma, Jintao Du, Changhua Meng, Weiqiang Wang, Jingwen Leng, Minyi Guo, <span style="font-weight: 500;">Zhouhan Lin#</span>
                            </p>
                            <div class="link-list">
                                <span>ACL 2025 (Main Conference) </span>
                                <span> | </span>
                                <a href="https://arxiv.org/abs/2502.11116"><span>arXiv:2502.11116</span></a>
                                <span> | </span>
                                <a href="https://aclanthology.org/2025.acl-long.354/"><span>Conference Link (ACL Anthology)</span></a>
                                <span> | </span>
                                <a href="https://github.com/LUMIA-Group/Gumbel-Reranking"><span>codes</span></a>
                            </div>
                            <abstract>
                                RAG systems rely on rerankers to identify relevant documents. however, fine-tuning these models remains challenging due to the lack of labeled query-document pairs. Existing distillation-based methods suffer from 
                                training-inference inconsistency and fail to capture inter-document dependencies among candidates. To overcome these limitations, we reframe the reranking process as an attention mask problem and propose Gumbel Reranking, 
                                an end-to-end training framework for rerankers designed to minimize the training-inference gap. In our approach, reranker optimization is reformulated as learning a stochastic, 
                                document-level Top-$k$ attention mask using the Gumbel Trick and Relaxed Top-$k$ Sampling. This formulation enables end-to-end optimization by minimizing the overall language loss. 
                                Experimental results across various settings consistently demonstrate performance gains, including a 10.4% recall improvement for distinguishing indirectly relevant documents on HotpotQA.
                            </abstract>
                        </div>
                    </div>

                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/cluster_gt.png" alt="Diagram illustrating the Cluster-wise Graph Transformer architecture with Node-to-Cluster Attention." srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Cluster-wise Graph Transformer with Dual-granularity Kernelized Attention
                            </h4>
                            <p class="author">
                                Siyuan Huang, Yunchong Song, Jiayue Zhou, <span style="font-weight: 500;">Zhouhan Lin#</span>
                            </p>
                            <div class="link-list">
                                <span>NeurIPS 2024 Spotlight </span>
                                <span> | </span>
                                <a href="https://arxiv.org/abs/2410.06746"><span>arXiv:2410.06746</span></a>
                                <span> | </span>
                                <span>Conference Link (NeurIPS 2024) - Please check the <a href="https://arxiv.org/abs/2410.06746"><span>arXiv page</span></a> for the official publication details.</span>
                                <span> | </span>
                                <a href="https://github.com/LUMIA-Group/Cluster-wise-Graph-Transformer"><span>codes</span></a>
                            </div>
                            <abstract>
                                In graph learning, one class of approaches conceptualizes the graph as a hierarchy, utilizing node clustering to capture broader structural information. While often effective, 
                                these methods typically rely on fixed graph coarsening routines, resulting in overly homogeneous cluster representations and a loss of node-level information. 
                                In this paper, we envision the graph as a network of interconnected node sets without compressing each cluster into a single embedding. To enable effective information transfer among these node sets, 
                                we propose the **Node-to-Cluster Attention (N2C-Attn)** mechanism. N2C-Attn incorporates techniques from Multiple Kernel Learning into a kernelized attention framework to effectively capture information at both the node and cluster levels. 
                                We then leverage a cluster-level message-passing framework to design an efficient form of N2C-Attn that achieves linear time complexity. We further analyze how N2C-Attn combines the dual-level feature maps of queries and keys, 
                                proving its ability to merge dual-granularity information. The resulting architecture, the **Cluster-wise Graph Transformer (Cluster-GT)**, which uses node clusters as tokens and employs our proposed N2C-Attn module, demonstrates superior performance on various graph-level tasks.
                            </abstract>
                        </div>
                    </div>

                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/subtree_attn.png" alt="Diagram showing the Subtree Attention mechanism for Graph Neural Networks." srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Tailoring Self-Attention for Graph via Rooted Subtrees
                            </h4>
                            <p class="author">
                                Siyuan Huang, Yunchong Song, Jiayue Zhou, <span style="font-weight: 500;">Zhouhan Lin#</span>
                            </p>
                            <div class="link-list">
                                <span>NeurIPS 2023 </span>
                                <span> | </span>
                                <a href="https://arxiv.org/abs/2310.05296"><span>arXiv:2310.05296</span></a>
                                <span> | </span>
                                <a href="https://openreview.net/forum?id=t2hEZadBBk"><span>Conference Link (OpenReview)</span></a>
                                <span> | </span>
                                <a href="https://github.com/LUMIA-Group/SubTree-Attention"><span>codes</span></a>
                            </div>
                            <abstract>
                                Attention mechanisms have driven significant advancements in graph learning, but they still have notable limitations: local attention struggles to capture long-range information due to the inherent message-passing mechanism, 
                                while global attention fails to reflect hierarchical neighborhood structures and cannot capture fine-grained local details. In this paper, we propose a novel multi-hop graph attention mechanism called **Subtree Attention (STA)** 
                                to address these issues. STA seamlessly connects the full attention structure with rooted subtrees and is theoretically proven to approximate global attention in extreme settings. By allowing direct computation of attention weights 
                                between multi-hop neighbors, STA alleviates the inherent problems of existing graph attention mechanisms. Furthermore, we design an efficient form of STA by employing kernelized Softmax, achieving linear time complexity. 
                                The resulting GNN architecture, **STAGNN**, presents a simple yet high-performing STA-based Graph Neural Network that utilizes a hop-aware attention strategy. Comprehensive evaluations on ten node classification datasets show that STA-based models outperform existing Graph Transformers and major GNNs.
                            </abstract>
                        </div>
                    </div>

                    <!-- research 1 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/orderedgnn.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Ordered GNN: Ordering Message Passing to Deal with Heterophily and Over-smoothing
                            </h4>
                            <p class="author">
                                Yunchong Song, Chenghu Zhou, Xinbing Wang, <span style="font-weight: 500;">Zhouhan Lin#</span>
                            </p>
                            <div class="link-list">
                                <span>ICLR 2023 </span>
                                <span> | </span>
                                <a href="https://arxiv.org/pdf/2302.01524.pdf"><span>pdf</span></a>
                                <span> | </span>
                                <a href="https://github.com/LUMIA-Group/OrderedGNN"><span>codes</span></a>
                            </div>
                            <abstract>
                                In this work, we propose to tackle both heterophily and over-smoothing problems by an ordered message passing mecanism, with specific blocks of neurons in a node embedding targeted for messages passed from neighboring nodes that are located within specific
                                hops. This is achieved by aligning the hierarchy of the rooted-tree of a central node with the ordered neurons in its node representation. SOTA performance in both homophily and heterophily settings without any targeted
                                design, robust to a wide number of layers, and explainable.
                        </div>
                    </div>

                    <!-- research 1 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/huref.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                HuRef: HUman-REadable Fingerprint for Large Language Models
                            </h4>
                            <p class="author">
                                Boyi Zeng, Lizheng Wang, Yuncong Hu, Yi Xu, Chenghu Zhou, Xinbing Wang, Yu Yu, <span style="font-weight: 500;">Zhouhan Lin#</span>
                            </p>
                            <div class="link-list">
                                <span>NeurIPS 2024 (Poster) </span>
                                <span> | </span>
                                <a href="https://arxiv.org/abs/2312.04828"><span>pdf</span></a>
                                <span> | </span>
                                <a href="https://github.com/LUMIA-Group/HuRef"><span>codes</span></a>
                            </div>
                            <abstract>
                                We introduce HuRef, a human-readable fingerprint designed to protect LLM copyright by identifying a model's original base version. Our study finds that while parameter vector directions remain stable after pretraining (persisting through SFT and RLHF), they are vulnerable to rotation or permutation attacks. To address this, we derive three invariant terms based on the Transformer structure. To prevent information leakage, these terms are mapped to a Gaussian vector and converted into a natural image via StyleGAN2. Finally, we utilize Zero-Knowledge Proofs (ZKP) to verify the authenticity of the generated fingerprint without exposing private model parameters, providing a secure and effective method for ownership verification.                    </div>

                    <!-- research 1 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/rasat.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                RASAT: Integrating Relational Structures into Pretrained Seq2Seq Model for Text-to-SQL
                            </h4>
                            <p class="author">
                                Jiexing Qi, Jingyao Tang, Ziwei He, Xiangpeng Wan, Yu Cheng, Chenghu Zhou, Xinbing Wang, Quanshi Zhang, <span style="font-weight: 500;">Zhouhan Lin#</span>
                            </p>
                            <div class="link-list">
                                <span>EMNLP 2022 </span>
                                <span> | </span>
                                <a href="https://arxiv.org/pdf/2205.06983.pdf"><span>pdf</span></a>
                                <span> | </span>
                                <a href="https://github.com/LUMIA-Group/rasat"><span>codes</span></a>
                            </div>
                            <abstract>
                                Relational structures such as schema linking and schema encoding have been validated as a key component to qualitatively translating natural language into SQL queries. We propose RASAT: a Transformer seq2seq architecture augmented with relation-aware
                                self-attention that could leverage a variety of relational structures while still being able to inherit the pretrained parameters from the T5 model effectively. Our model can incorporate almost all types of existing relations
                                in the literature. Experimental results on three widely used text-to-SQL datasets, covering both single-turn and multi-turn scenarios, have shown that RASAT could achieve state-of-the-art results across all three benchmarks
                                (75.5% EX on Spider, 52.6% IEX on SParC, and 37.4% IEX on CoSQL).
                        </div>
                    </div>

                    <!-- research 1 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/distanceparser.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Straight to the Tree: Constituency Parsing with Neural Syntactic Distance
                            </h4>
                            <p class="author">
                                Yikang Shen*, <span style="font-weight: 500;">Zhouhan Lin</span>*, Athul Paul Jacob, Alessandro Sordoni, Aaron Courville, Yoshua Bengio
                            </p>
                            <div class="link-list">
                                <span>ACL 2018 </span>
                                <span> | </span>
                                <a href="https://arxiv.org/pdf/1806.04168.pdf"><span>pdf</span></a>
                                <span> | </span>
                                <a href="http://"><span>slides</span></a>
                                <span> | </span>
                                <a href="https://github.com/hantek/distance-parser"><span>codes</span></a>
                            </div>
                            <abstract>
                                We propose a novel constituency parsing scheme. The model predicts a vector of real-valued scalars, named syntactic distances, for each split position in the input sentence. The syntactic distances specify the order in which the split points will be selected,
                                recursively partitioning the input, in a top-down fashion. Compared to traditional shiftreduce parsing schemes, our approach is free from the potential problem of compounding errors, while being faster and easier to parallelize.
                                Our model achieves competitive performance amongst single model, discriminative parsers in the PTB dataset and outperforms previous models in the CTB dataset.
                            </abstract>
                        </div>
                    </div>

                    <!-- research 3 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/tree.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Neural Language Modeling by Jointly Learning Syntax and Lexicon
                            </h4>
                            <p class="author">
                                Yikang Shen, <span style="font-weight: 500;">Zhouhan Lin</span>, Chin-Wei Huang, Aaron Courville
                            </p>
                            <div class="link-list">
                                <span>ICLR 2018</span>
                                <span> | </span>
                                <a href="https://arxiv.org/pdf/1711.02013.pdf"><span>pdf</span></a>
                                <span> | </span>
                                <a href="http://https://github.com/yikangshen/PRPN"><span>codes</span></a>
                                <!-- <span> | </span> -->
                                <!-- <a href=""><span>poster</span></a> -->
                            </div>
                            <abstract>
                                We propose a neural language model capable of unsupervised syntactic structure induction. The model leverages the structure information to form better semantic representations and better language modeling. Standard recurrent neural networks are limited
                                by their structure and fail to efficiently use syntactic information. On the other hand, tree-structured recursive networks usually require additional structural supervision at the cost of human expert annotation. In this
                                paper, We propose a novel neural language model, called the Parsing-Reading-Predict Networks (PRPN), that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure
                                to learn a better language model. In our model, the gradient can be directly back-propagated from the language model loss into the neural parsing network. Experiments show that the proposed model can discover the underlying
                                syntactic structure and achieve state-of-the-art performance on word/character-level language model tasks.
                            </abstract>
                        </div>
                    </div>
                    <!-- research 4 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/semlp.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                A structured self-attentive Sentence Embedding
                            </h4>
                            <p class="author">
                                <span style="font-weight: 500;">Zhouhan Lin</span>, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou and Yoshua Bengio
                            </p>
                            <div class="link-list">
                                <span class="time">ICLR 2017</span>
                                <span> | </span>
                                <a href="https://arxiv.org/pdf/1703.03130.pdf"><span>pdf</span></a>
                                <span> | </span>
                                <a href="https://github.com/hantek/SelfAttentiveSentEmbed"><span>codes</span></a>
                                <!-- <span> | </span> -->
                                <!-- <a href=""><span>poster</span></a> -->
                            </div>
                            <abstract>
                                We propose a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence.
                                We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the
                                embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification, and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding
                                methods in all of the 3 tasks.
                            </abstract>
                        </div>
                    </div>

                    <!-- research 5 -->
                    <div class="row">
                        <div class="col-xs-12 col-sm-4">
                            <img width="100%" src="./assets/research/exp_quant.png" alt="" srcset="">
                        </div>
                        <div class="col-xs-12 col-sm-8">
                            <h4>
                                Neural networks with few multiplications
                            </h4>
                            <p class="author">
                                <span style="font-weight: 500;">Zhouhan Lin</span>, Matthieu Courbariaux, Roland Memisevic, and Yoshua Bengio
                            </p>
                            <div class="link-list">
                                <span class="time">ICLR 2016 (oral)</span>
                                <span> | </span>
                                <a href="https://arxiv.org/pdf/1510.03009.pdf"><span>pdf</span></a>
                                <span> | </span>
                                <a href="https://github.com/hantek/BinaryConnect"><span>codes</span></a>
                                <!-- <span> | </span> -->
                                <!-- <a href=""><span>poster</span></a> -->
                            </div>
                            <abstract>
                                For most deep learning algorithms training is notoriously time consuming. Since most of the computation in training neural networks is typically spent on floating point multiplications, we investigate an approach to training that eliminates the need for
                                most of these. Our method consists of two parts: First we stochastically binarize weights to convert multiplications involved in computing hidden states to sign changes. Second, while back-propagating error derivatives,
                                in addition to binarizing the weights, we quantize the representations at each layer to convert the remaining multiplications into binary shifts. Experimental results across 3 popular datasets (MNIST, CIFAR10, SVHN) show
                                that this approach not only does not hurt classification performance but can result in even better performance than standard stochastic gradient descent training, paving the way to fast, hardwarefriendly training of neural
                                networks.
                            </abstract>
                        </div>
                    </div>

                    <!-- 在这里新建一个新的research，可复制一段research代码，从class=row开始, 格式如下 -->
                    <!-- <div class="row">……</div> -->
                </section>

            </section>

        </section>
    </div>
    <!-- jQuery文件。务必在bootstrap.min.js 之前引入 -->
    <script src="https://cdn.staticfile.org/jquery/2.1.1/jquery.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js" crossorigin="anonymous"></script>
    <script src="./index.js"></script>
</body>

</html>
